# -*- coding: utf-8 -*-
"""Untitled13.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1sc6mGcP-YLst97Rvz7l7mq_kgdmlgaUI
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import re

# Load raw datasets
esg_df = pd.read_csv("esg_scores_2021.csv")
financial_df = pd.read_csv("filtered_security_data.csv")
price_df = pd.read_csv("snp500_stocks_closing_price_daily_data.csv")

# Merge ESG + Financial
merged_metadata = pd.merge(esg_df, financial_df, on="symbol", how="inner")
merged_all = pd.merge(merged_metadata, price_df, on="symbol", how="inner")

# Drop columns with all nulls
merged_all.dropna(axis=1, how="all", inplace=True)

# Identify price columns (e.g., 01-Mar-21)
price_columns = [col for col in merged_all.columns if re.match(r"\d{1,2}-[A-Za-z]{3}-\d{2}", col)]

# Remove text-heavy columns
text_columns = [
    'symbol', 'esgPerformance', 'peerGroup', 'longBusinessSummary', 'address1', 'address2', 'city', 'state', 'country',
    'phone', 'website', 'companyOfficers', 'uuid', 'logo_url', 'messageBoardId', 'market', 'exchange', 'shortName',
    'longName', 'exchangeTimezoneName', 'exchangeTimezoneShortName', 'quoteType', 'underlyingSymbol',
    'underlyingExchangeSymbol', 'headSymbol', 'category'
]

candidate_numeric = merged_all.drop(columns=text_columns + price_columns, errors='ignore')
numeric_features = candidate_numeric.select_dtypes(include=['int64', 'float64'])

# Fill NaNs with column mean, then scale
numeric_features = numeric_features.fillna(numeric_features.mean())
scaler = StandardScaler()
scaled_features = scaler.fit_transform(numeric_features)

# Rebuild scaled metadata
symbols = merged_all['symbol'].reset_index(drop=True)
scaled_metadata = pd.DataFrame(scaled_features, columns=numeric_features.columns)
scaled_metadata.insert(0, "symbol", symbols)

# Prepare price matrix (transposed)
price_data = merged_all[price_columns]
price_data_t = price_data.T
price_data_t.columns = symbols
price_data_t.index.name = "date"

# Drop dates with too many NaNs, then fill forward/backward
threshold = int(0.1 * price_data_t.shape[1])
price_data_t = price_data_t.dropna(thresh=price_data_t.shape[1] - threshold)
price_data_t = price_data_t.ffill().bfill()

# Final check
assert not price_data_t.isnull().any().any(), "NaNs remain in price data!"
assert not scaled_metadata.isnull().any().any(), "NaNs remain in metadata!"

# Save
scaled_metadata.to_csv("scaled_metadata.csv", index=False)
price_data_t.to_csv("price_matrix.csv")

print("✅ Cleaned & saved: scaled_metadata.csv, price_matrix.csv")

import pandas as pd
import numpy as np
from datetime import timedelta
from dateutil.relativedelta import relativedelta
from sklearn.preprocessing import StandardScaler
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from matplotlib import pyplot as plt

import scipy.optimize as sco

def sharpe_optimization(expected_returns, cov_matrix):
    num_assets = len(expected_returns)

    def portfolio_performance(weights):
        port_return = np.dot(weights, expected_returns)
        port_volatility = np.sqrt(np.dot(weights.T, np.dot(cov_matrix, weights)))
        return -port_return / port_volatility  # Negative Sharpe Ratio (we minimize)

    constraints = {'type': 'eq', 'fun': lambda x: np.sum(x) - 1}
    bounds = tuple((0, 1) for _ in range(num_assets))
    initial_weights = np.array([1. / num_assets] * num_assets)

    result = sco.minimize(portfolio_performance, initial_weights, method='SLSQP',
                          bounds=bounds, constraints=constraints)

    return result.x if result.success else initial_weights


# Load datasets
metadata = pd.read_csv("scaled_metadata.csv")
price_df = pd.read_csv("price_matrix.csv", index_col=0)
price_df.index = pd.to_datetime(price_df.index, format="%d-%b-%y", errors='coerce')
price_df = price_df.sort_index()

sp500 = pd.read_csv("snp500_INDEX_daily_closing_prices.csv")
sp500["date"] = pd.to_datetime(sp500["Date"])
sp500 = sp500.set_index("date").sort_index()
sp500_prices = sp500["Close Price"]

symbols = list(set(metadata["symbol"]).intersection(price_df.columns))
metadata = metadata[metadata["symbol"].isin(symbols)]
price_df = price_df[symbols]

lookback_days = 90
prediction_days = 60
start_date = price_df.index[0] + timedelta(days=lookback_days)
end_date = price_df.index[-1] - timedelta(days=prediction_days)
rebalance_dates = pd.date_range(start=start_date, end=end_date, freq="QS")

portfolio_value = 1.0
benchmark_value = 1.0
cumulative_returns = []
benchmark_returns = []
dates = []

# Utility

def get_return(prices, start, end):
    return (prices.loc[end] - prices.loc[start]) / prices.loc[start]

def safe_price(date):
    dates = sp500_prices.index[sp500_prices.index >= date]
    return sp500_prices.loc[dates[0]] if not dates.empty else None

# Loop
for i in range(len(rebalance_dates) - 1):
    train_start = rebalance_dates[i] - timedelta(days=lookback_days)
    train_end = rebalance_dates[i] - timedelta(days=1)
    test_start = rebalance_dates[i]
    test_end = rebalance_dates[i + 1] - timedelta(days=1)

    try:
        past_prices = price_df.loc[train_start:train_end]
        future_prices = price_df.loc[test_start:test_end]

        past_return = get_return(past_prices, past_prices.index[0], past_prices.index[-1])
        future_return = get_return(future_prices, future_prices.index[0], future_prices.index[-1])

        train_df = metadata.copy()
        train_df["past_return"] = train_df["symbol"].map(past_return)
        train_df["target_return"] = train_df["symbol"].map(future_return)
        train_df = train_df.dropna(subset=["past_return", "target_return"])

        if train_df.shape[0] < 30:
            print(f"⚠️ Skipping {test_start.date()} — insufficient data.")
            continue

        X_train = train_df.drop(columns=["symbol", "target_return"]).values
        y_train = train_df["target_return"].values

        model = Sequential([
            Dense(128, activation='relu', input_shape=(X_train.shape[1],)),
            Dropout(0.3),
            Dense(64, activation='relu'),
            Dropout(0.3),
            Dense(1)
        ])
        model.compile(optimizer='adam', loss='mse')
        model.fit(X_train, y_train, epochs=100, verbose=0)

        predict_df = metadata.copy()
        predict_df["past_return"] = predict_df["symbol"].map(past_return)
        predict_df = predict_df.dropna(subset=["past_return"])
        X_pred = predict_df.drop(columns=["symbol"]).values
        predict_df["score"] = model.predict(X_pred).flatten()

        top_stocks = predict_df.sort_values("score", ascending=False).head(20)
        top_symbols = top_stocks["symbol"].values

        future_window = future_prices[top_symbols]
        top_returns = future_window.pct_change().dropna()
        cov_matrix = top_returns.cov()
        expected_returns = past_return[top_symbols].values

        weights = sharpe_optimization(expected_returns, cov_matrix)
        mean_returns = top_returns.mean()
        vol = top_returns.std()
        sharpe_scores = mean_returns / vol
        weights = sharpe_scores / sharpe_scores.sum()

        daily_returns = top_returns @ weights.values
        cumulative = (1 + daily_returns).cumprod() * portfolio_value
        portfolio_value = cumulative[-1]

        sp_start, sp_end = safe_price(test_start), safe_price(test_end)
        if sp_start is None or sp_end is None:
            print(f"⚠️ Missing S&P 500 data from {test_start.date()} to {test_end.date()}, skipping.")
            continue

        sp_return = (sp_end - sp_start) / sp_start
        benchmark_value *= (1 + sp_return)

        cumulative_returns.append(portfolio_value)
        benchmark_returns.append(benchmark_value)
        dates.append(test_end.date())

        print(f"✅ {test_start.date()} → {test_end.date()} | Portfolio: {cumulative[-1]:.2f} | S&P 500: {benchmark_value:.2f}")

    except Exception as e:
        print(f"❌ Error from {test_start.date()}: {e}")
        continue

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(dates, cumulative_returns, label="📈 NN Portfolio")
plt.plot(dates, benchmark_returns, label="🏛️ S&P 500 Index")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.title("Neural Network Portfolio vs S&P 500")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# CAGR
if dates:
    def cagr(start, end, years): return (end / start) ** (1 / years) - 1
    years = (dates[-1] - dates[0]).days / 365.25
    port_cagr = cagr(cumulative_returns[0], cumulative_returns[-1], years)
    bench_cagr = cagr(benchmark_returns[0], benchmark_returns[-1], years)
    print(f"\n📈 Portfolio CAGR: {port_cagr:.2%}")
    print(f"🏛️ S&P 500 Index CAGR: {bench_cagr:.2%}")


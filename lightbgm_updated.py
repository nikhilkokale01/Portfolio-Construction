# -*- coding: utf-8 -*-
"""lightbgm_updated.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1HuLAcMdqL6wOXbPaQ8jaK65IQU41_VWG
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
import re

# Load raw datasets
esg_df = pd.read_csv("esg_scores_2021.csv")
financial_df = pd.read_csv("filtered_security_data.csv")
price_df = pd.read_csv("snp500_stocks_closing_price_daily_data.csv")

# Merge ESG + Financial
merged_metadata = pd.merge(esg_df, financial_df, on="symbol", how="inner")
merged_all = pd.merge(merged_metadata, price_df, on="symbol", how="inner")

# Drop columns with all nulls
merged_all.dropna(axis=1, how="all", inplace=True)

# Identify price columns (e.g., 01-Mar-21)
price_columns = [col for col in merged_all.columns if re.match(r"\d{1,2}-[A-Za-z]{3}-\d{2}", col)]

# Remove text-heavy columns
text_columns = [
    'symbol', 'esgPerformance', 'peerGroup', 'longBusinessSummary', 'address1', 'address2', 'city', 'state', 'country',
    'phone', 'website', 'companyOfficers', 'uuid', 'logo_url', 'messageBoardId', 'market', 'exchange', 'shortName',
    'longName', 'exchangeTimezoneName', 'exchangeTimezoneShortName', 'quoteType', 'underlyingSymbol',
    'underlyingExchangeSymbol', 'headSymbol', 'category'
]

candidate_numeric = merged_all.drop(columns=text_columns + price_columns, errors='ignore')
numeric_features = candidate_numeric.select_dtypes(include=['int64', 'float64'])

# Fill NaNs with column mean, then scale
numeric_features = numeric_features.fillna(numeric_features.mean())
scaler = StandardScaler()
scaled_features = scaler.fit_transform(numeric_features)

# Rebuild scaled metadata
symbols = merged_all['symbol'].reset_index(drop=True)
scaled_metadata = pd.DataFrame(scaled_features, columns=numeric_features.columns)
scaled_metadata.insert(0, "symbol", symbols)

# Prepare price matrix (transposed)
price_data = merged_all[price_columns]
price_data_t = price_data.T
price_data_t.columns = symbols
price_data_t.index.name = "date"

# Drop dates with too many NaNs, then fill forward/backward
threshold = int(0.1 * price_data_t.shape[1])
price_data_t = price_data_t.dropna(thresh=price_data_t.shape[1] - threshold)
price_data_t = price_data_t.ffill().bfill()

# Final check
assert not price_data_t.isnull().any().any(), "NaNs remain in price data!"
assert not scaled_metadata.isnull().any().any(), "NaNs remain in metadata!"

# Save
scaled_metadata.to_csv("scaled_metadata.csv", index=False)
price_data_t.to_csv("price_matrix.csv")

print("‚úÖ Cleaned & saved: scaled_metadata.csv, price_matrix.csv")

import pandas as pd
import numpy as np
import lightgbm as lgb
import matplotlib.pyplot as plt
from sklearn.preprocessing import KBinsDiscretizer
from datetime import timedelta
from dateutil.relativedelta import relativedelta
from scipy.optimize import minimize

# Load datasets
metadata = pd.read_csv("scaled_metadata.csv")
price_df = pd.read_csv("price_matrix.csv", index_col=0)
price_df.index = pd.to_datetime(price_df.index, format="%d-%b-%y", errors='coerce')
price_df = price_df.sort_index()

sp500 = pd.read_csv("snp500_INDEX_daily_closing_prices.csv")
sp500["date"] = pd.to_datetime(sp500["Date"])
sp500 = sp500.set_index("date").sort_index()
sp500_prices = sp500["Close Price"]

# Get intersection of symbols
symbols = list(set(metadata["symbol"]).intersection(price_df.columns))
metadata = metadata[metadata["symbol"].isin(symbols)]
price_df = price_df[symbols]

# Parameters
lookback_days = 90
prediction_days = 60
start_date = price_df.index[0] + timedelta(days=lookback_days)
end_date = price_df.index[-1] - timedelta(days=prediction_days)
rebalance_dates = pd.date_range(start=start_date, end=end_date, freq="QS")

# Tracking results
portfolio_value = 1.0
benchmark_value = 1.0
cumulative_returns = []
benchmark_returns = []
dates = []

# Define utility functions
def get_return_matrix(prices, start, end):
    return (prices.loc[end] - prices.loc[start]) / prices.loc[start]

def sharpe_optimization(expected_returns, cov_matrix):
    def neg_sharpe(w):
        ret = np.dot(w, expected_returns)
        vol = np.sqrt(np.dot(w.T, np.dot(cov_matrix, w)))
        return -ret / vol if vol != 0 else 0
    x0 = np.repeat(1/len(expected_returns), len(expected_returns))
    bounds = [(0, 1)] * len(expected_returns)
    constraints = {'type': 'eq', 'fun': lambda w: np.sum(w) - 1}
    result = minimize(neg_sharpe, x0, bounds=bounds, constraints=constraints)
    return result.x

# Backtest loop
for i in range(len(rebalance_dates) - 1):
    train_start = rebalance_dates[i] - timedelta(days=lookback_days)
    train_end = rebalance_dates[i] - timedelta(days=1)
    test_start = rebalance_dates[i]
    test_end = rebalance_dates[i + 1] - timedelta(days=1)

    try:
        past_prices = price_df.loc[train_start:train_end]
        future_prices = price_df.loc[test_start:test_end]

        # Returns
        past_ret = get_return_matrix(past_prices, past_prices.index[0], past_prices.index[-1])
        future_ret = get_return_matrix(future_prices, future_prices.index[0], future_prices.index[-1])

        train_df = metadata.copy()
        train_df["past_return"] = train_df["symbol"].map(past_ret)
        train_df["target_return"] = train_df["symbol"].map(future_ret)
        train_df = train_df.dropna(subset=["past_return", "target_return"])

        if train_df.shape[0] < 30:
            print(f"‚ö†Ô∏è Skipping {test_start.date()} ‚Äî insufficient data.")
            continue

        # Training
        X_train = train_df.drop(columns=["symbol", "target_return"])
        y_train = KBinsDiscretizer(n_bins=5, encode="ordinal").fit_transform(train_df[["target_return"]]).ravel()
        lgb_train = lgb.Dataset(X_train.values, label=y_train, group=[len(X_train)])

        model = lgb.train({
            "objective": "lambdarank",
            "metric": "ndcg",
            "verbosity": -1
        }, lgb_train, num_boost_round=100)

        # Prediction: use same metadata + past return, but not target return!
        predict_df = metadata.copy()
        predict_df["past_return"] = predict_df["symbol"].map(past_ret)
        predict_df = predict_df.dropna(subset=["past_return"])
        X_pred = predict_df.drop(columns=["symbol"])

        scores = model.predict(X_pred.values)
        predict_df["score"] = scores
        top_stocks = predict_df.sort_values("score", ascending=False).head(20)
        top_symbols = top_stocks["symbol"].values

        print(f"Iteration loop {i}")
        print(predict_df.shape)
        print(predict_df[['score','symbol']])

        # Portfolio weights (Sharpe optimization)
        future_price_window = future_prices[top_symbols]
        top_returns = future_price_window.pct_change().dropna()
        cov_matrix = top_returns.cov()
        # expected_returns = future_ret[top_symbols].values
        expected_returns = past_ret[top_symbols].values
        weights = sharpe_optimization(expected_returns, cov_matrix)

        port_return = np.dot(weights, expected_returns)
        transaction_cost = 0.01  # 0.1%
        portfolio_value *= (1 + port_return)


        # Benchmark return
        def safe_price(date):
            dates = sp500_prices.index[sp500_prices.index >= date]
            return sp500_prices.loc[dates[0]] if not dates.empty else None

        sp_start, sp_end = safe_price(test_start), safe_price(test_end)
        if sp_start is None or sp_end is None:
            print(f"‚ö†Ô∏è Missing S&P 500 data from {test_start.date()} to {test_end.date()}, skipping.")
            continue

        sp_return = (sp_end - sp_start) / sp_start
        benchmark_value *= (1 + sp_return)

        cumulative_returns.append(portfolio_value)
        benchmark_returns.append(benchmark_value)
        dates.append(test_end.date())

        print(f"‚úÖ {test_start.date()} ‚Üí {test_end.date()} | Portfolio: {port_return:.2%} | S&P 500: {sp_return:.2%}")

    except Exception as e:
        print(f"‚ùå Error from {test_start.date()}: {e}")
        continue

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(dates, cumulative_returns, label="üìà Optimized Portfolio")
plt.plot(dates, benchmark_returns, label="üèõÔ∏è S&P 500 Index")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.title("Realistic Portfolio vs S&P 500 (LightGBM Ranker)")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# CAGR Calculation
def cagr(start, end, years): return (end / start) ** (1 / years) - 1
if dates:
    years = (dates[-1] - dates[0]).days / 365.25
    port_cagr = cagr(cumulative_returns[0], cumulative_returns[-1], years)
    bench_cagr = cagr(benchmark_returns[0], benchmark_returns[-1], years)
    print(f"\nüìà Portfolio CAGR: {port_cagr:.2%}")
    print(f"üèõÔ∏è S&P 500 Index CAGR: {bench_cagr:.2%}")

# Tracking results
portfolio_value = 1.0
benchmark_value = 1.0
previous_weights = np.zeros_like(weights)  # Initialize previous weights
cumulative_returns = []
benchmark_returns = []
dates = []

# Backtest loop
for i in range(len(rebalance_dates) - 1):
    train_start = rebalance_dates[i] - timedelta(days=lookback_days)
    train_end = rebalance_dates[i] - timedelta(days=1)
    test_start = rebalance_dates[i]
    test_end = rebalance_dates[i + 1] - timedelta(days=1)

    try:
        past_prices = price_df.loc[train_start:train_end]
        future_prices = price_df.loc[test_start:test_end]

        # Returns
        past_ret = get_return_matrix(past_prices, past_prices.index[0], past_prices.index[-1])
        future_ret = get_return_matrix(future_prices, future_prices.index[0], future_prices.index[-1])

        train_df = metadata.copy()
        train_df["past_return"] = train_df["symbol"].map(past_ret)
        train_df["target_return"] = train_df["symbol"].map(future_ret)
        train_df = train_df.dropna(subset=["past_return", "target_return"])

        if train_df.shape[0] < 30:
            print(f"‚ö†Ô∏è Skipping {test_start.date()} ‚Äî insufficient data.")
            continue

        # Training
        X_train = train_df.drop(columns=["symbol", "target_return"])
        y_train = KBinsDiscretizer(n_bins=5, encode="ordinal").fit_transform(train_df[["target_return"]]).ravel()
        lgb_train = lgb.Dataset(X_train.values, label=y_train, group=[len(X_train)])

        model = lgb.train({
            "objective": "lambdarank",
            "metric": "ndcg",
            "verbosity": -1
        }, lgb_train, num_boost_round=100)

        # Prediction: use same metadata + past return, but not target return!
        predict_df = metadata.copy()
        predict_df["past_return"] = predict_df["symbol"].map(past_ret)
        predict_df = predict_df.dropna(subset=["past_return"])
        X_pred = predict_df.drop(columns=["symbol"])

        scores = model.predict(X_pred.values)
        predict_df["score"] = scores
        top_stocks = predict_df.sort_values("score", ascending=False).head(20)
        top_symbols = top_stocks["symbol"].values

        # Portfolio weights (Sharpe optimization)
        future_price_window = future_prices[top_symbols]
        top_returns = future_price_window.pct_change().dropna()
        cov_matrix = top_returns.cov()
        expected_returns = past_ret[top_symbols].values  # Fixing the issue here with expected returns

        weights = sharpe_optimization(expected_returns, cov_matrix)

        # Calculate weight change and transaction cost
        weight_change = np.abs(weights - previous_weights)
        transaction_cost = np.sum(weight_change * portfolio_value) * 0.01  # 1% transaction cost

        # Portfolio return with transaction cost
        port_return = np.dot(weights, expected_returns)
        portfolio_value *= (1 + port_return)  # Apply portfolio return
        portfolio_value -= transaction_cost  # Subtract transaction cost

        # Update previous weights
        previous_weights = weights

        # Benchmark return
        sp_start, sp_end = safe_price(test_start), safe_price(test_end)
        if sp_start is None or sp_end is None:
            print(f"‚ö†Ô∏è Missing S&P 500 data from {test_start.date()} to {test_end.date()}, skipping.")
            continue

        sp_return = (sp_end - sp_start) / sp_start
        benchmark_value *= (1 + sp_return)

        cumulative_returns.append(portfolio_value)
        benchmark_returns.append(benchmark_value)
        dates.append(test_end.date())

        print(f"‚úÖ {test_start.date()} ‚Üí {test_end.date()} | Portfolio: {port_return:.2%} | S&P 500: {sp_return:.2%}")

    except Exception as e:
        print(f"‚ùå Error from {test_start.date()}: {e}")
        continue

# Plotting
plt.figure(figsize=(10, 6))
plt.plot(dates, cumulative_returns, label="üìà Optimized Portfolio")
plt.plot(dates, benchmark_returns, label="üèõÔ∏è S&P 500 Index")
plt.xlabel("Date")
plt.ylabel("Cumulative Return")
plt.title("Realistic Portfolio vs S&P 500 (LightGBM Ranker)")
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

# CAGR Calculation
def cagr(start, end, years): return (end / start) ** (1 / years) - 1
if dates:
    years = (dates[-1] - dates[0]).days / 365.25
    print(f"Date end: {dates[-1]}")
    print(f"Date start: {dates[0]}")
    port_cagr = cagr(cumulative_returns[0], cumulative_returns[-1], years)
    bench_cagr = cagr(benchmark_returns[0], benchmark_returns[-1], years)
    print(f"\nüìà Portfolio CAGR: {port_cagr:.2%}")
    print(f"üèõÔ∏è S&P 500 Index CAGR: {bench_cagr:.2%}")